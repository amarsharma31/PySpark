spark.stop()
sc.stop()

from pyspark.sql import SparkSession
from pyspark.sql import Row

spark = SparkSession.builder\
.appName("MDR Test - Amar")\
.config("spark.dynamicAllocation.enabled", "true")\
.config("spark.shuffle.service.enabled", "true")\
.config("spark.executor.cores","5")\
.config("spark.executor.memory", "8G")\
.config("spark.driver.memory", "12G")\
.config('spark.dynamicAllocation.maxExecutors','10') \
.getOrCreate()

import os
import sys

import pandas as pd
from pyspark.sql import SparkSession, SQLContext

sc = spark.sparkContext
sqlContext = SQLContext(sc)
#spark.conf.set('spark.sql.pivotMaxValues', u'50000000')

from pyspark.sql import Window
from pyspark.sql import functions as f
from pyspark.sql.functions import col, first

#windowSpec = Window.partitionBy("new", "dataitemvalue")

df = spark.read.parquet("hdfs:///tmp/amar/data/data1/dex.parquet")
df1 = df.groupBy("index_name","calendar_date","calendar_year","calendar_month","calendar_quarter").agg(f.mean("index_value"))
df1 = df1.select('*').withColumn("Dummy", f.lit("Dummy"))
df2 = df1.groupBy("calendar_date","calendar_year","calendar_month","calendar_quarter").pivot("index_name").agg({'avg(index_value)': 'mean'}).drop('Dummy')
df2.show()
